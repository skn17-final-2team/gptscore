import os, torch, platform, json 
from dotenv import load_dotenv
from huggingface_hub import login

from peft import PeftModel
from langchain.vectorstores import FAISS
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.embeddings import HuggingFaceEmbeddings

from typing import Any, List, Optional
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.prompt_values import PromptValue
from langchain_core.messages import BaseMessage


os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

# ===== ëª¨ë¸ ì„¤ì • =====
base_model_name = "Qwen/Qwen3-8B"
ft_model_name = "CHOROROK/Qwen3_8B_meeting_agenda_task"

# # ===== ì´ìŠ¤ì¼€ì´íŠ¸ =====
# def escape_curly(text: str) -> str:
#     return text.replace("{", "{{").replace("}", "}}")

# ===== ì „ë¬¸ ì „ì²˜ë¦¬ (JSON -> í…ìŠ¤íŠ¸) =====
def preprocess_transcript(transcript):
    # 1) ì´ë¯¸ íŒŒì´ì¬ ê°ì²´(list/dict)ë¡œ ë“¤ì–´ì˜¨ ê²½ìš°
    if isinstance(transcript, list):
        data = transcript
    elif isinstance(transcript, dict):
        data = [transcript]
    else:
        # 2) ë¬¸ìì—´ì´ë©´ JSON íŒŒì‹± ì‹œë„
        try:
            data = json.loads(transcript)
        except Exception:
            return str(transcript)

    lines = []
    for entry in data:
        if isinstance(entry, dict):
            for speaker, text in entry.items():
                lines.append(f"{speaker}: {text}")
        else:
            lines.append(str(entry))

    return "\n\n".join(lines)
        
# ===== ë²¡í„°DB ë¡œë“œ =====
def load_faiss_db(db_path: str):
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ ê³ ë ¤)
    embedding_model = HuggingFaceEmbeddings(
        model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko",
        encode_kwargs={'normalize_embeddings': True}  # ì •ê·œí™”ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
    )
    vector_store = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    print("ğŸ”µ FAISS DB ë¡œë“œ ì™„ë£Œ!\n")
    return vector_store, embedding_model


# ===== í˜•ì‹ì§€ì • =====
class HFTextGenLLM(LLM):
    """HF text-generation pipelineì„ ê°ì‹¸ëŠ”, ë¹„-ìŠ¤íŠ¸ë¦¬ë° LLM ë˜í¼."""
    pipe: Any
    tokenizer: Any = None
    model: Any = None
    model_config = {"arbitrary_types_allowed": True}

    @property
    def _llm_type(self) -> str:
        return "hf_text_generation_pipeline"

    def _normalize_prompt(self, prompt) -> str:
        """strë¡œ ì •ê·œí™”í•˜ê³  ìœ ë‹ˆì½”ë“œ ë¬¸ì œ í•´ê²°."""
        result = ""

        if isinstance(prompt, PromptValue): # prompt template > string
            result = prompt.to_string()
        elif isinstance(prompt, list) and prompt:  # chatmessage list > contentë§Œ ì¶”ì¶œ
            if isinstance(prompt[0], BaseMessage):
                result = "\n".join(m.content for m in prompt)
            else:
                result = str(prompt)
        elif isinstance(prompt, str): # string 
            result = prompt
        else:
            result = str(prompt)

        # ì„œë¡œê²Œì´íŠ¸ ë¬¸ì ì œê±° (ì—¬ê¸°ì„œ ë¨¼ì € ì²˜ë¦¬) : ë©”ëª¨ì¥ ê¹¨ì§€ëŠ” ê±° ì²˜ë¦¬ 
        try:
            result = result.encode('utf-8', errors='surrogateescape').decode('utf-8', errors='ignore')
        except:
            # í´ë°±: ì„œë¡œê²Œì´íŠ¸ë¥¼ ë¬´ì‹œí•˜ê³  ì¬ì¸ì½”ë”©
            result = result.encode('utf-16', 'surrogatepass').decode('utf-16', errors='ignore')
        return result


    def _call(self, prompt, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs) -> str:
        text = self._normalize_prompt(prompt)

        if not isinstance(text, str): # str ë³€í™˜
            print(f"âš ï¸ ê²½ê³ : í”„ë¡¬í”„íŠ¸ê°€ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤. íƒ€ì…: {type(text)}")
            text = str(text)

        print(f"ğŸ”¹ LLM ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(text)} ê¸€ì")
        text = str(text).strip()         # ë¬¸ìì—´ íƒ€ì… ê°•ì œ í™•ì¸ ë° ì •ë¦¬ & ê³µë°±ì œê±°

        # ë„ˆë¬´ ê¸´ ì…ë ¥ì€ ì˜ë¼ë‚´ê¸° (í† í° ê¸°ì¤€ ì•½ 6000ê°œ) : ì•ˆì „ì¥ì¹˜ (ì•ì— ë‚´ìš©ë§Œ ë°˜í™˜)
        MAX_INPUT_CHARS = 12000
        if len(text) > MAX_INPUT_CHARS:
            print(f"âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(text)}ì). {MAX_INPUT_CHARS}ìë¡œ ìë¦…ë‹ˆë‹¤.")
            text = text[:MAX_INPUT_CHARS]

        # HF text-generation pipeline ì‹¤í–‰
        try:
            if self.tokenizer and self.model: # qwen chat template
                print("ğŸ”¹ ì§ì ‘ tokenizer/model ì‚¬ìš© ëª¨ë“œ")
                import torch
                try:
                    print(f"   Tokenizer íƒ€ì…: {type(self.tokenizer)}")
                    messages = [{"role": "user", "content": text}]

                    if hasattr(self.tokenizer, 'apply_chat_template'):  # chat template
                        print("   apply_chat_template ì‚¬ìš©")
                        formatted_text = self.tokenizer.apply_chat_template(
                            messages,
                            tokenize=False,
                            add_generation_prompt=True
                        )
                    else:
                        print("   chat template ì—†ìŒ, ì§ì ‘ ì‚¬ìš©")  # ì§ì ‘ ìƒì„±
                        formatted_text = text

                    # í† í¬ë‚˜ì´ì¦ˆ
                    inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt",
                        truncation=True,
                        max_length=6144
                    )
                    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                    print(f"   í† í¬ë‚˜ì´ì¦ˆ ì„±ê³µ: inputs shape = {inputs['input_ids'].shape}")


                except Exception as tok_error:
                    print(f"   âŒ Tokenizer í˜¸ì¶œ ì‹¤íŒ¨: {tok_error}")
                    print(f"   í´ë°±: pipeline ì‚¬ìš©")
                    raise  # í´ë°±í•˜ë„ë¡ ì—ëŸ¬ ë°œìƒì‹œí‚´

                # ìƒì„±
                with torch.no_grad():
                    output_ids = self.model.generate(
                        **inputs,
                        max_new_tokens=2048,
                        temperature=0.2,
                        top_p=0.9,
                        do_sample=True
                    )

                # ë””ì½”ë”© (ìƒˆë¡œ ìƒì„±ëœ ê²ƒë§Œ)
                generated_text = self.tokenizer.decode(
                    output_ids[0][inputs['input_ids'].shape[1]:],
                    skip_special_tokens=True
                )
                return generated_text
            else:
                outputs = self.pipe(text)  # tokenizer/model ì§ì ‘ ì‚¬ìš© ë¶ˆê°€ ì‹œ, ê¸°ì¡´ pipeline ì‚¬ìš©.
                if not outputs:
                    return ""

                first = outputs[0]
                generated = first.get("generated_text") or first.get("text") or ""

                if stop:
                    for s in stop:
                        if s in generated:
                            generated = generated.split(s)[0]
                            break

                return generated

        except Exception as e:
            print(f"âŒ Pipeline ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ!")
            print(f"   ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {e}")
            print(f"   ì…ë ¥ íƒ€ì…: {type(text)}")
            print(f"   ì…ë ¥ ê¸¸ì´: {len(text) if isinstance(text, str) else 'N/A'}")
            print(f"   ì…ë ¥ ìƒ˜í”Œ: {text[:200] if isinstance(text, str) else text}...")

            # ì¶”ê°€ ë””ë²„ê¹…: ì…ë ¥ì˜ repr í™•ì¸
            print(f"   ì…ë ¥ repr: {repr(text[:100])}")
            raise



# ===== ëª¨ë¸ ë¡œë“œ =====
def load_model_q(model_name: str | None = base_model_name , adapter_name: str | None = ft_model_name):

    # Fast tokenizer ë¹„í™œì„±í™” (TextEncodeInput ì—ëŸ¬ ë°©ì§€)
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        print("ğŸ”¹ Slow tokenizer ì‚¬ìš©")
    except Exception:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("ğŸ”¹ Fast tokenizer ì‚¬ìš© (í´ë°±)")

    # ê¸´ ì…ë ¥ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
    if tokenizer.model_max_length > 100000:  # ë¹„ì •ìƒì ìœ¼ë¡œ í° ê°’ì¸ ê²½ìš°
        tokenizer.model_max_length = 8192    # í•©ë¦¬ì ì¸ ê°’ìœ¼ë¡œ ì„¤ì •
    print(f"ğŸ”¹ Tokenizer max_length: {tokenizer.model_max_length}")
    
    if platform.system() == "Windows":
        print("âš  Windowsì—ì„œëŠ” 4bit ë¶ˆê°€ â†’ FP16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map='auto'
        )
    else:
        print("ğŸ”µ Linux/RunPod í™˜ê²½: 4bit ì—†ì´ bf16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,   
            device_map="auto",
        )

    if adapter_name:
        print(f"ğŸ”µ LoRA/PEFT ì–´ëŒ‘í„° ë¡œë“œ: {adapter_name}")
        model = PeftModel.from_pretrained(base_model, adapter_name)
    else:
        model = base_model

    text_gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=False,
        max_new_tokens=2048,
        temperature=0.2,
        top_p=0.9
    )

    llm = HFTextGenLLM(pipe=text_gen_pipe, tokenizer=tokenizer, model=model)
    return llm

